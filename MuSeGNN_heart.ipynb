{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b316837-2a44-4e18-b53f-6f24abb9173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch_geometric.nn\n",
    "import torch_geometric.data as data\n",
    "import networkx as nx\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "\n",
    "from torch_geometric.nn import TransformerConv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f44ae2a-5280-4a40-9626-85380362e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Run encoder')\n",
    "\n",
    "    parser.add_argument('--epoches', type=int, default=2000,\n",
    "                        help='number of epoches')\n",
    "    parser.add_argument('--lambdac', type=float, default=0.01,\n",
    "                        help='weight for the contrastive learning') \n",
    "    parser.add_argument('--lr1', type=float, default=1e-4,\n",
    "                        help='lr for encoder')     \n",
    "    parser.add_argument('--lr2', type=float, default=1e-3,\n",
    "                        help='lr for decoder')\n",
    "    parser.add_argument('--temp', type=float, default=0.07,\n",
    "                        help='temperature for the contrastive learning') \n",
    "    parser.add_argument('--samplesize', type=int, default=100,\n",
    "                        help='sample size for contrastive learning')\n",
    "    parser.add_argument('--dim', type=float, default=32,\n",
    "                        help='latent dimensions') \n",
    "    parser.add_argument('--savepath', type=str, default=\"heart_global/heart_umi_m_musegnn.h5ad\",\n",
    "                        help='save path') \n",
    "    \n",
    "    \n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c25ed5d4-9078-4ac3-9f6f-dd695b8fab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(epoches=3, lambdac=0.01, lr1=0.0001, lr2=0.001, temp=0.07, samplesize=100, dim=32.0, savepath='heart_umi_m_musegnn.h5ad')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#set_seed(0)\n",
    "\n",
    "# # for specific encoder/decoder\n",
    "# # tissue_list = { \n",
    "# #                \"heart\":[233, 676, 783, 947,266, 223, 233, 978, 928, 852, 839, 733]}\n",
    "sys.argv = [\n",
    "    'script_name',  # The first element is the script name, can be any name.\n",
    "    '--epoches', '3',\n",
    "    '--lambdac', '0.01',\n",
    "    '--lr1', '1e-4',\n",
    "    '--lr2', '1e-3',\n",
    "    '--temp', '0.07',\n",
    "    '--samplesize', '100',\n",
    "    '--dim', '32',\n",
    "    '--savepath', 'heart_umi_m_musegnn.h5ad'\n",
    "]\n",
    "\n",
    "# Now, you can call your function and parse the arguments\n",
    "args = parse_args()\n",
    "\n",
    "# Display the arguments to confirm that they are parsed correctly\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "56cb2c3f-afc0-4826-a27a-5590497fe8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_list = { \n",
    "               \"scrna_heart\":['D4',\n",
    " 'H2',\n",
    " 'H3',\n",
    " 'D6',\n",
    " 'D2',\n",
    " 'H7',\n",
    " 'D11',\n",
    " 'D3',\n",
    " 'D1',\n",
    " 'D5',\n",
    " 'H4',\n",
    " 'D7',\n",
    " 'H6',\n",
    " 'H5',\n",
    " 'G19'], \n",
    "#                    \"scrna_pancreas\":['2017', 'bTop3'], \n",
    "#          \"scrna_kidney\":[\"b1\", \"b2\"],\n",
    "               }\n",
    "\n",
    "tissue_list = {\"heart\":['rna', \"spatial\"]}\n",
    "\n",
    "# construct graph batch\n",
    "# based on simulation results\n",
    "graph_list = []\n",
    "cor_list = []\n",
    "label_list = []\n",
    "graph_networkx_list = []\n",
    "count = 0\n",
    "gene_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b140b6e7-265b-419f-8f85-dd290ae8f2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'heart': ['rna', 'spatial']}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tissue_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c06db3b-8e10-4f66-b657-d4c949b37e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rna\n",
      "(1000, 1000)\n",
      "(1000, 5000)\n",
      "spatial\n",
      "(1000, 1000)\n",
      "(1000, 5000)\n"
     ]
    }
   ],
   "source": [
    "for tissue in tissue_list.keys():\n",
    "    for i in tissue_list[tissue]:\n",
    "        print(i)\n",
    "        #pathway_count = \"/projectnb/cs598/projects/singleCell/data/heart_spatial_expression.csv\"\n",
    "        #pathway_matrix = \"/projectnb/cs598/projects/singleCell/data/heart_spatial_pvalue.csv\"\n",
    "        pathway_count = \"/projectnb/cs598/projects/singleCell/data/heart_\" + i + \"_expression.csv\"\n",
    "        pathway_matrix = \"/projectnb/cs598/projects/singleCell/data/heart_\" + i + \"_pvalue.csv\"\n",
    "\n",
    "        pd_adata_new =  pd.read_csv(pathway_count, index_col=0)\n",
    "        correlation = pd.read_csv(pathway_matrix, index_col=0)\n",
    "        cor_list.append(correlation)\n",
    "\n",
    "        print(correlation.shape)\n",
    "        print(pd_adata_new.shape)\n",
    "        adata = sc.AnnData(pd_adata_new)\n",
    "        gene_length = len(adata)\n",
    "\n",
    "        adata_new = adata.copy()\n",
    "        edges_new = np.array([np.nonzero(correlation.values)[0],np.nonzero(correlation.values)[1]])\n",
    "        graph = data.Data(x=torch.FloatTensor(adata_new.X.copy()), edge_index=torch.FloatTensor(edges_new).long())\n",
    "        \n",
    "        vis = nx.from_pandas_adjacency(correlation)\n",
    "        graph_networkx_list.append(vis)\n",
    "        \n",
    "        graph.gene_list = pd_adata_new.index\n",
    "        graph.show_index = tissue +\"__\" + str(i)\n",
    "        \n",
    "        graph_list.append(graph)\n",
    "        label_list.append(tissue)\n",
    "        \n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "025d386f-33e5-419a-8be4-8a4fada5cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder_Multiinput(torch.nn.Module):\n",
    "    def __init__(self, out_channels, graph_list, label_list):\n",
    "        super(GCNEncoder_Multiinput, self).__init__()\n",
    "        self.activ = nn.Mish(inplace=True)\n",
    "        \n",
    "        conv_dict = {}\n",
    "        conv_dict1 = {}\n",
    "        for i in graph_list:\n",
    "            conv_dict[i.show_index] = torch_geometric.nn.Sequential('x, edge_index', [(TransformerConv(i.x.shape[1], out_channels, heads=4),'x, edge_index-> x'),\n",
    "                                                     (torch_geometric.nn.GraphNorm(out_channels*4), 'x -> x')])\n",
    "        self.convl1 = nn.ModuleDict(conv_dict)\n",
    "        self.convl2 = nn.ModuleDict(conv_dict1)\n",
    "              \n",
    "    def forward(self, x, edge_index, show_index):\n",
    "        x = self.convl1[show_index](x, edge_index)\n",
    "        x = self.activ(x)\n",
    "        return x\n",
    "    \n",
    "class GCNEncoder_Commoninput(torch.nn.Module):\n",
    "    def __init__(self, out_channels, graph_list, label_list):\n",
    "        super(GCNEncoder_Commoninput, self).__init__()\n",
    "        self.activ = nn.Mish(inplace=True)\n",
    "        \n",
    "        conv_dict_l2 = {}\n",
    "        conv_dict_l3 = {}\n",
    "        conv_dict_l4 = {}\n",
    "        tissue_specific_list = list(set(label_list))\n",
    "        \n",
    "        for i in tissue_specific_list:\n",
    "            conv_dict_l2[i] = torch_geometric.nn.Sequential('x, edge_index', [(TransformerConv(out_channels*4, out_channels, heads=2),'x, edge_index -> x'),\n",
    "                                                     (torch_geometric.nn.GraphNorm(out_channels*2), 'x -> x')])\n",
    "            conv_dict_l3[i] = TransformerConv(out_channels*2, out_channels)\n",
    "            conv_dict_l4[i] = TransformerConv(out_channels*4, out_channels)\n",
    "              \n",
    "        self.convl2 = nn.ModuleDict(conv_dict_l2)\n",
    "        self.convl3 = nn.ModuleDict(conv_dict_l3)\n",
    "        self.convl4 = nn.ModuleDict(conv_dict_l4)\n",
    "        \n",
    "    def forward(self, x, edge_index, show_index):\n",
    "        x_inp = x\n",
    "        x = self.convl2[show_index.split('__')[0]](x, edge_index)\n",
    "        x = self.activ(x)\n",
    "        x = self.convl3[show_index.split('__')[0]](x, edge_index)\n",
    "        return x + self.convl4[show_index.split('__')[0]](x_inp, edge_index)\n",
    "\n",
    "class MLP_edge_Decoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, graph_list):\n",
    "        super(MLP_edge_Decoder, self).__init__()\n",
    "        \n",
    "        dec_dict = {}\n",
    "        \n",
    "        self.activ = nn.Mish(inplace=True)\n",
    "        for i in graph_list:\n",
    "            dec_dict[i.show_index] = torch.nn.Sequential(\n",
    "                                              nn.Linear(in_channels,  out_channels)\n",
    "                                             , self.activ,\n",
    "                                              nn.Linear(in_channels,  out_channels)\n",
    "                                             , self.activ,\n",
    "                                              nn.Linear(in_channels,  out_channels)\n",
    "                                             )\n",
    "        self.MLP = nn.ModuleDict(dec_dict)\n",
    "        \n",
    "    def forward(self, x, show_index):\n",
    "        x = self.MLP[show_index](x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bcb02e1d-80a9-4904-bda5-c8184bd86e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[1000, 5000], edge_index=[2, 311364], gene_list=Index(['ENSG00000187608', 'ENSG00000171621', 'ENSG00000162444',\n",
       "        'ENSG00000175206', 'ENSG00000120937', 'ENSG00000189337',\n",
       "        'ENSG00000231606', 'ENSG00000188257', 'ENSG00000173372',\n",
       "        'ENSG00000159189',\n",
       "        ...\n",
       "        'ENSG00000154734', 'ENSG00000156273', 'ENSG00000156299',\n",
       "        'ENSG00000159216', 'ENSG00000157554', 'ENSG00000198888',\n",
       "        'ENSG00000198899', 'ENSG00000198840', 'ENSG00000198886',\n",
       "        'ENSG00000198727'],\n",
       "       dtype='object', length=1000), show_index='heart__rna'),\n",
       " Data(x=[1000, 5000], edge_index=[2, 206998], gene_list=Index(['ENSG00000188290', 'ENSG00000187608', 'ENSG00000162576',\n",
       "        'ENSG00000179403', 'ENSG00000197785', 'ENSG00000189409',\n",
       "        'ENSG00000235169', 'ENSG00000116285', 'ENSG00000074800',\n",
       "        'ENSG00000171819',\n",
       "        ...\n",
       "        'ENSG00000159200', 'ENSG00000185437', 'ENSG00000157601',\n",
       "        'ENSG00000160214', 'ENSG00000160255', 'ENSG00000182871',\n",
       "        'ENSG00000142156', 'ENSG00000142173', 'ENSG00000228253',\n",
       "        'ENSG00000212907'],\n",
       "       dtype='object', length=1000), show_index='heart__spatial')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b558b2fb-63d3-46b8-9e40-1e1f52abe8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish preprocessing\n",
      "start training\n"
     ]
    }
   ],
   "source": [
    "gene_encoder_is = GCNEncoder_Multiinput(int(args.dim), graph_list, label_list).to(device)\n",
    "gene_encoder_com =  GCNEncoder_Commoninput(int(args.dim), graph_list, label_list).to(device)\n",
    "\n",
    "gene_decoder = MLP_edge_Decoder(gene_length, gene_length ,graph_list).to(device)\n",
    "\n",
    "optimizer_enc_is = torch.optim.Adam(gene_encoder_is.parameters(), lr=args.lr1)\n",
    "optimizer_enc_com = torch.optim.Adam(gene_encoder_com.parameters(), lr=args.lr1)\n",
    "\n",
    "optimizer_dec2 = torch.optim.Adam(gene_decoder.parameters(), lr=args.lr2)\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from concurrent.futures import ProcessPoolExecutor,as_completed, ThreadPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import process_pair\n",
    "\n",
    "def compute_gene_sets(graph_list, graph_networkx_list, num_threads=cpu_count()):\n",
    "    common_gene_set = {}\n",
    "    common_gene_overlap = {}\n",
    "    diff_gene_set = {}\n",
    "    diff_gene_neighbor_set = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_pair.process_pair, i, j, graph_list, graph_networkx_list)\n",
    "            for i in range(len(graph_list))\n",
    "            for j in range(len(graph_list))\n",
    "            if i != j\n",
    "        ]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            i, j, common_set, common_overlap, diff_set, diff_neighbor_set = future.result()\n",
    "            key = graph_list[i].show_index + graph_list[j].show_index\n",
    "            common_gene_set[key] = common_set\n",
    "            common_gene_overlap[key] = common_overlap\n",
    "            diff_gene_set[key] = diff_set\n",
    "            diff_gene_neighbor_set[key] = diff_neighbor_set\n",
    "\n",
    "    return common_gene_set, common_gene_overlap, diff_gene_set, diff_gene_neighbor_set\n",
    "common_gene_set, common_gene_overlap, diff_gene_set, diff_gene_neighbor_set = compute_gene_sets(graph_list, graph_networkx_list)\n",
    "\n",
    "print(\"finish preprocessing\")\n",
    "print(\"start training\")\n",
    "\n",
    "loss_f = nn.BCEWithLogitsLoss()\n",
    "loss_m = nn.CrossEntropyLoss()\n",
    "from pytorch_metric_learning import losses\n",
    "loss_func = losses.SelfSupervisedLoss(losses.NTXentLoss(temperature = args.temp))\n",
    "\n",
    "lambda_infonce = args.lambdac\n",
    "def penalize_data(z, graph_list,j):\n",
    "    loss = torch.tensor(0.).to(device)\n",
    "    graph_new = graph_list[j]\n",
    "\n",
    "    x = graph_new.x.to(device)\n",
    "    train_pos_edge_index = graph_new.edge_index.to(device)\n",
    "    \n",
    "    x = gene_encoder_is(x, train_pos_edge_index, graph_new.show_index)\n",
    "    z_new = gene_encoder_com(x, train_pos_edge_index, graph_new.show_index)\n",
    "\n",
    "    [index_i, index_j] = common_gene_set[graph.show_index + graph_new.show_index]\n",
    "    if (len(index_i) ==0) or (len(index_j) == 0):\n",
    "        return loss\n",
    "    \n",
    "    z_cor = z[index_i]\n",
    "    z_new_cor = z_new[index_j]\n",
    "    \n",
    "    weight = torch.FloatTensor(common_gene_overlap[graph.show_index + graph_new.show_index]).to(device)\n",
    "    cos_sim = torch.cosine_similarity(z_cor, z_new_cor, axis = 1)*weight\n",
    "\n",
    "    loss += -cos_sim.mean()\n",
    "    \n",
    "    [index_i, index_j] = diff_gene_set[graph.show_index + graph_new.show_index]\n",
    "\n",
    "    if (len(index_i) ==0) or (len(index_j) == 0):\n",
    "        return loss\n",
    "    \n",
    "    opt_index = np.random.choice([i for i in range(len(index_i))], min(100, len(index_i)))\n",
    "    \n",
    "    z_diff = z[index_i[opt_index]]\n",
    "    z_new_diff = z_new[index_j[opt_index]]\n",
    "    \n",
    "    [index_i, index_j] = diff_gene_neighbor_set[graph.show_index + graph_new.show_index]\n",
    "    \n",
    "    z_diff_true =  z[index_i[opt_index]]\n",
    "    z_new_diff_true = z_new[index_j[opt_index]]\n",
    "    \n",
    "    loss += lambda_infonce * loss_func(torch.cat((z_diff,z_new_diff)), torch.cat((z_diff_true,z_new_diff_true)))\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "97b9a7da-9c09-42bd-9ef1-69c90a12a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7113894820213318\n",
      "0.7253931164741516\n",
      "epoch finish\n",
      "epoch finish\n",
      "epoch finish\n"
     ]
    }
   ],
   "source": [
    "# Contrastive learning\n",
    "gene_encoder_is.train()\n",
    "gene_encoder_com.train()\n",
    "graph_index_list = [item for item in range(0, len(graph_list))]\n",
    "edge_adj_list = [torch.FloatTensor(cor_list[i].values).to(device) for i in graph_index_list]\n",
    "\n",
    "for epoch in range(args.epoches):\n",
    "    loss = 0.\n",
    "    for i in range(0,len(graph_index_list)):\n",
    "        \n",
    "        optimizer_enc_is.zero_grad(set_to_none=True)\n",
    "        optimizer_enc_com.zero_grad(set_to_none=True)\n",
    "        optimizer_dec2.zero_grad(set_to_none=True)\n",
    "\n",
    "        graph = graph_list[i]\n",
    "\n",
    "        x = graph.x.to(device)\n",
    "        train_pos_edge_index = graph.edge_index.to(device)\n",
    "        edge_adj = edge_adj_list[i]\n",
    "\n",
    "        x = gene_encoder_is(x, train_pos_edge_index,  graph.show_index)\n",
    "        z = gene_encoder_com(x, train_pos_edge_index, graph.show_index)\n",
    "        \n",
    "        adj = torch.matmul(z, z.t())\n",
    "        edge_reconstruct = gene_decoder(adj, graph.show_index)\n",
    "        \n",
    "        loss = loss_f(edge_reconstruct.flatten(), edge_adj.flatten()) \n",
    "\n",
    "        if epoch % 200 ==0:\n",
    "            print(loss.item())\n",
    "\n",
    "        graph_index_list_copy = graph_index_list.copy()\n",
    "        graph_index_list_copy.remove(i)\n",
    "\n",
    "        j = random.sample(graph_index_list_copy, 1)\n",
    "        loss += penalize_data(z, graph_list,j[0]) \n",
    "\n",
    "        del graph\n",
    "        loss.backward()\n",
    "        del loss\n",
    "        \n",
    "        optimizer_enc_is.step()\n",
    "        optimizer_enc_com.step()\n",
    "        optimizer_dec2.step()\n",
    "        \n",
    "    print(\"epoch finish\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "emb_list = []\n",
    "gene_list = []\n",
    "tissue_list = []\n",
    "\n",
    "# inference step\n",
    "gene_encoder_is.eval()\n",
    "gene_encoder_com.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0,len(graph_list)):\n",
    "        graph = graph_list[i]\n",
    "        x = graph.x.to(device)\n",
    "        train_pos_edge_index = graph.edge_index.long().to(device)\n",
    "        \n",
    "        x = gene_encoder_is(x, train_pos_edge_index,  graph.show_index)\n",
    "        z = gene_encoder_com(x, train_pos_edge_index, graph.show_index)\n",
    "        \n",
    "        emb_list.append(z.cpu().numpy())\n",
    "        gene_list.append(graph.gene_list)\n",
    "        tissue_list.append([graph.show_index for j in range(len(x))])\n",
    "\n",
    "adata = sc.AnnData(np.concatenate(emb_list))\n",
    "\n",
    "adata.obs['gene'] = np.concatenate(gene_list)\n",
    "adata.obs['tissue'] = np.concatenate(tissue_list)\n",
    "\n",
    "sc.pp.neighbors(adata, use_rep='X')\n",
    "sc.tl.umap(adata)\n",
    "\n",
    "tissue_list1 = []\n",
    "for i in list(set(adata.obs['tissue'])):\n",
    "    tissue_list1.append(list(adata[adata.obs['tissue'] == i].obs['gene']))\n",
    "\n",
    "common_gene_list = set(tissue_list1[0]).intersection(*tissue_list1[1:])\n",
    "\n",
    "adata.obs['displaygene']= [True if i in common_gene_list else False for i in adata.obs['gene']]\n",
    "adata.obs['displaygene']  = adata.obs['displaygene'].astype('category')\n",
    "\n",
    "sc.tl.leiden(adata)\n",
    "\n",
    "adata.obs['tissue_new'] = [i.split(\"__\")[0] for i in adata.obs['tissue']]\n",
    "\n",
    "adata.write_h5ad(args.savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94847f89-4ad4-466b-b032-d1d1b98a0468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           heart__rna\n",
       "1           heart__rna\n",
       "2           heart__rna\n",
       "3           heart__rna\n",
       "4           heart__rna\n",
       "             ...      \n",
       "1995    heart__spatial\n",
       "1996    heart__spatial\n",
       "1997    heart__spatial\n",
       "1998    heart__spatial\n",
       "1999    heart__spatial\n",
       "Name: tissue, Length: 2000, dtype: category\n",
       "Categories (2, object): ['heart__rna', 'heart__spatial']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sc.pp.neighbors(adata, use_rep='X')\n",
    "#sc.tl.umap(adata)\n",
    "adata.obs['tissue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5be17a5d-b647-4b50-b883-8030a5eb2bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart__rna\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/projectnb/cs598/projects/singleCell/data/heart_heart__rna_pvalue.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m pathway_count = \u001b[33m\"\u001b[39m\u001b[33m/projectnb/cs598/projects/singleCell/data/heart_\u001b[39m\u001b[33m\"\u001b[39m + i + \u001b[33m\"\u001b[39m\u001b[33m_expression.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m pathway_matrix = \u001b[33m\"\u001b[39m\u001b[33m/projectnb/cs598/projects/singleCell/data/heart_\u001b[39m\u001b[33m\"\u001b[39m + i + \u001b[33m\"\u001b[39m\u001b[33m_pvalue.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m correlation = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathway_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m cor_list[tissue +\u001b[33m\"\u001b[39m\u001b[33m__\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(i)] = correlation\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(correlation.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rprojectnb/apoe-signatures/aleshchk/.conda/envs/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rprojectnb/apoe-signatures/aleshchk/.conda/envs/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rprojectnb/apoe-signatures/aleshchk/.conda/envs/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rprojectnb/apoe-signatures/aleshchk/.conda/envs/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/rprojectnb/apoe-signatures/aleshchk/.conda/envs/myenv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/projectnb/cs598/projects/singleCell/data/heart_heart__rna_pvalue.csv'"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import scib\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# construct graph batch based on simulation results\n",
    "graph_list = []\n",
    "cor_list = {}\n",
    "label_list = [] \n",
    "count = 0\n",
    "for tissue in tissue_list.keys():\n",
    "    for i in tissue_list[tissue]:\n",
    "        print(i)\n",
    "        pathway_count = \"/projectnb/cs598/projects/singleCell/data/heart_\" + i + \"_expression.csv\"\n",
    "        pathway_matrix = \"/projectnb/cs598/projects/singleCell/data/heart_\" + i + \"_pvalue.csv\"\n",
    "        correlation = pd.read_csv(pathway_matrix, sep=\",\", index_col=0)\n",
    "        cor_list[tissue +\"__\" + str(i)] = correlation\n",
    "\n",
    "        print(correlation.shape)\n",
    "\n",
    "        label_list.append(tissue +\"__\" + str(i))\n",
    "        \n",
    "        count +=1\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# metric 1: Calculate gene ASW\n",
    "def calculate_common_asw(adata):\n",
    "    tissue_list = []\n",
    "    for i in list(set(adata.obs['tissue'])):\n",
    "        tissue_list.append(list(adata[adata.obs['tissue'] == i].obs['gene']))\n",
    "        \n",
    "    common_gene_list = set(tissue_list[0]).intersection(*tissue_list[1:])\n",
    "    adata_new = adata[[True if i in common_gene_list else False for i in adata.obs['gene']]]\n",
    "    adata_new.obsm['X_emb'] = adata_new.X\n",
    "    \n",
    "    result = scib.metrics.silhouette_batch(adata_new, batch_key='tissue', group_key='leiden', embed='X_emb')\n",
    "    \n",
    "    return result    \n",
    "\n",
    "\n",
    "# metric 2: Calculate normalized AUC\n",
    "def calculate_AUC(adata, cor_list):\n",
    "    tissue_list = list(set(cor_list.keys()))\n",
    "    \n",
    "    result = 0\n",
    "    for i in tissue_list:\n",
    "        adata_new = adata[adata.obs['tissue'] == i]\n",
    "\n",
    "        normed_matrix = normalize(adata_new.X, axis=1)\n",
    "        rec_matrix = sigmoid(normed_matrix@normed_matrix.T).flatten()\n",
    "        cor_matrix = cor_list[i].values.flatten()\n",
    "        result += roc_auc_score(cor_matrix, rec_matrix)\n",
    "    \n",
    "    result = result/len(tissue_list)\n",
    "    return result    \n",
    "\n",
    "# metric 3: Calculate gene LISI\n",
    "def calculate_iLISI(adata):\n",
    "    tissue_list = []\n",
    "    for i in list(set(adata.obs['tissue'])):\n",
    "        tissue_list.append(list(adata[adata.obs['tissue'] == i].obs['gene']))\n",
    "        \n",
    "    common_gene_list = set(tissue_list[0]).intersection(*tissue_list[1:])\n",
    "    adata_new = adata[[True if i in common_gene_list else False for i in adata.obs['gene']]]\n",
    "    adata_new.obsm['X_emb'] = adata_new.X\n",
    "    \n",
    "    result = scib.metrics.ilisi_graph(adata_new, batch_key=\"tissue\", type_=\"embed\")\n",
    "    \n",
    "    return result   \n",
    "\n",
    "# metric 4: Calculate gene GC\n",
    "def calculate_graph_connectivity(adata):\n",
    "    tissue_list = []\n",
    "    for i in list(set(adata.obs['tissue'])):\n",
    "        tissue_list.append(list(adata[adata.obs['tissue'] == i].obs['gene']))\n",
    "        \n",
    "    common_gene_list = set(tissue_list[0]).intersection(*tissue_list[1:])\n",
    "    adata_new = adata[[True if i in common_gene_list else False for i in adata.obs['gene']]]\n",
    "\n",
    "    adata_new.obsm['X_emb'] = adata_new.X\n",
    "    \n",
    "    result = scib.metrics.graph_connectivity(adata_new,'leiden')\n",
    "    \n",
    "    return result    \n",
    "\n",
    "# metric 5: Calculate commom gene propertion\n",
    "def calculate_common_gene_propertion(adata):\n",
    "    full_score = 0\n",
    "    for i in list(set(adata.obs['leiden'])):\n",
    "        adata_new = adata[adata.obs['leiden'] == i]\n",
    "        \n",
    "        gene_list = set(adata_new.obs['gene'])\n",
    "        \n",
    "        prop = 1 - len(gene_list)/len(adata_new.obs['gene'])\n",
    "        print(prop)\n",
    "        \n",
    "        full_score += len(adata_new)/len(adata) * prop\n",
    "        \n",
    "    return full_score\n",
    "\n",
    "# Helper function: Jaccard score for genes in different graphs.\n",
    "def calculate_overlap(G1,G2,g1,g2):\n",
    "    G1_neg = list(G1.neighbors(g1))\n",
    "    G2_neg = list(G2.neighbors(g2))\n",
    "    \n",
    "    overlap_score = len(set(G1_neg).intersection(set(G2_neg)))/len(set(G1_neg).union(set(G2_neg)))\n",
    "    \n",
    "    return overlap_score\n",
    "\n",
    "# metric 6: Calculate neighbor genes' overlap\n",
    "def calculate_common_neighbor_ovarlap(adata, cor_list):\n",
    "    output_value = 0\n",
    "    for i in list(set(adata.obs['leiden'])):\n",
    "        adata_new = adata[adata.obs['leiden'] == i]\n",
    "        \n",
    "        tissue_list = list(adata_new.obs['tissue'])\n",
    "        gene_list = list(adata_new.obs['gene'])\n",
    "        \n",
    "        overlap_value = 0\n",
    "        dim_value = 0\n",
    "        for num1,item1 in enumerate(gene_list):\n",
    "            for num2,item2 in enumerate(gene_list):\n",
    "                t1 = tissue_list[num1]\n",
    "                t2 = tissue_list[num2]\n",
    "                if t1 != t2:\n",
    "                    g1 = graph_list[t1]\n",
    "                    g2 = graph_list[t2]\n",
    "                    temp_overlap = calculate_overlap(g1,g2,item1,item2)\n",
    "                    overlap_value += temp_overlap\n",
    "                    dim_value += 1.0\n",
    "                \n",
    "        print(\"finish one cluster\")\n",
    "        \n",
    "        if dim_value == 0:\n",
    "            overlap_value = 0\n",
    "        else:\n",
    "            overlap_value = overlap_value/dim_value\n",
    "        output_value += overlap_value*len(adata_new)/len(adata)\n",
    "        \n",
    "    return output_value\n",
    "\n",
    "# Integrated function for metric calculation\n",
    "def calculate_metric(adata, cor_list):\n",
    "    asw = calculate_common_asw(adata)\n",
    "    AUC = calculate_AUC(adata, cor_list)\n",
    "    #ilisi = calculate_iLISI(adata)\n",
    "    gc = calculate_graph_connectivity(adata)\n",
    "    \n",
    "    \n",
    "    ratio = calculate_common_gene_propertion(adata)\n",
    "    \n",
    "    ovl = calculate_common_neighbor_ovarlap(adata, cor_list)\n",
    "    \n",
    "    #df = pd.DataFrame(np.array([asw,AUC,ilisi,gc,ratio, ovl]))\n",
    "    df = pd.DataFrame(np.array([asw, AUC,gc,ratio, ovl]))\n",
    "    #df.index = ['ASW', 'AUC', 'iLISI', 'GC', 'Common ratio', 'share overlap']\n",
    "    df.index = ['ASW', 'AUC', 'GC', 'Common ratio', 'share overlap']\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "# run the benchmark process\n",
    "graph_list = {}\n",
    "for i in cor_list.keys():\n",
    "    graph_list[i] = nx.from_pandas_adjacency(cor_list[i])\n",
    "\n",
    "# run the benchmark\n",
    "seed = 0\n",
    "adata = sc.read_h5ad(\"heart_umi_m_musegnn.h5ad\")\n",
    "calculate_metric(adata, cor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4752525e-f11a-4e0c-aaf7-dc7262d2d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_list = {\"heart\":['heart__rna', \"heart__spatial\"]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
